---
layout: single
title: "Linear Regression vs Logistic Regression"
category: machine-learning
tag: ML
toc: true
author_profile: false
sidebar:
    nav: "docs"
mathjax: true
---

<div class="notice--info">
<h4>Please have a read.</h4>
<ul>
	<li>[Welcome!](https://dae-y.github.io/notice/first_post/)</li>
</ul>
</div>

## A Quick Table for Linear Regression vs Logistic Regression

| Feature             | Linear Regression               | Logistic Regression (Classification) |
|---------------------|--------------------------------|--------------------------------------|
| **Data (Output)**   | Continuous Values             | Discrete Categories (Probabilities) |
| **Loss Function**   | Mean Squared Error (MSE)      | Cross-Entropy Loss (Binary)         |
| **Optimization**    | Gradient Descent              | Gradient Descent                    |
| **Classification**  | Not Applicable               | Probability-based (Threshold 0.5)   |
| **Primary Use**     | Prediction of Continuous Values | Classification (Binary or Multi-class) |

---

## Why is Classification Called 'Regression'?

Although **Logistic Regression** contains the word *regression*, it is actually used for classification. The reason for this name comes from its foundation in **linear regression**, but with a key modification:  

- Instead of predicting continuous values, **Logistic Regression outputs probabilities.**  
- The final decision is made by applying a **threshold (typically 0.5):**  
  - If the output is **≥ 0.5**, classify as **positive (1)**  
  - If the output is **< 0.5**, classify as **negative (0)**  

This probability-based classification method differentiates **logistic regression from traditional linear regression**.

---

## Introduction to Classification

Classification is a fundamental concept in machine learning where the goal is to assign input data to predefined categories. Unlike regression, which predicts continuous values, **classification outputs discrete labels** (e.g., cat vs. dog, spam vs. not spam).  

Some common classification problems:
- Email filtering (spam vs. non-spam)
- Disease diagnosis (positive vs. negative)
- Image recognition (dog vs. cat)

---

## Logistic Regression: The Basics

### Why Not Use Linear Regression for Classification?
Linear regression is not ideal for classification because its output is **unbounded** (i.e., it can take any value from \(-\infty\) to \(+\infty\)).  
To restrict the output to a probability range **[0,1]**, we use the **sigmoid function**:

$$
y = \frac{1}{1 + e^{-(wx + b)}}
$$

### How the Sigmoid Function Works:
- The sigmoid function **maps any real number** to a value between **0 and 1**.
- This allows us to interpret the output as a probability.
- A **threshold (usually 0.5)** is used to classify the result.

---

## Training Logistic Regression

### Why Mean Squared Error (MSE) is Not Used
Using **MSE (Mean Squared Error)** for logistic regression results in a **non-convex cost function**, leading to local minima and poor convergence.  
Instead, we use the **logistic loss function (binary cross-entropy loss):**

$$
L = -\left[ y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right]
$$

### Optimization Using Gradient Descent
- **Gradient Descent** is used to adjust the weights \(w\) and bias \(b\) to minimize the loss.
- The **learning rate** determines how quickly the model updates its parameters.
- Choosing the right learning rate is crucial to avoid overshooting or slow convergence.

---

## Support Vector Machines (SVM)

### What is SVM?
SVM is another linear classification algorithm that aims to find the **optimal decision boundary** between classes.  
Instead of just fitting a line, **SVM maximizes the margin** between the closest points (support vectors).

### Key Concepts:
- **Decision boundary**: A hyperplane that separates different classes.
- **Support vectors**: Data points that are closest to the decision boundary.
- **Margin**: The distance between the support vectors and the decision boundary.
- **Hard margin SVM**: Enforces a strict separation between classes.
- **Trigger variable \( Y_i \)**: Helps determine class labels in optimization.

---

## Summary of Key Concepts

| Concept             | Definition |
|---------------------|------------|
| **Decision Boundary** | The line or hyperplane that separates different classes. |
| **Support Vectors** | Data points closest to the decision boundary. |
| **Margin** | The distance between the decision boundary and the closest data points. |
| **Gradient Descent** | An optimization algorithm used to train machine learning models. |
| **Loss Function** | Measures how well the model’s predictions match actual values. |
| **Hyperparameters** | Parameters set before training (e.g., learning rate). |

---

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

