---
layout: single
title: "Linear Regression and Gradient Descent"
category: machine-learning
tag: ML
author_profile: false
mathjax: true
sidebar:
    nav: "counts"
---

A summary of COMP3010 Machine learning lecture 2 <br>
and extra notes on Gradient Descent

## Linear Regression Overview
In **linear regression**, we aim to find the best-fitting line to predict a target value \( y \) from an input \( x \). The model can be expressed as:

$$
f(x) = wx + b
$$

Where:
- \( w \) is the **weight** (or slope of the line),
- \( b \) is the **bias** (or y-intercept),
- \( f(x) \) is the **predicted value** for a given input \( x \).

The goal is to minimize the difference between the predicted values \( f(x) \) and the actual target values \( y \).

## Gradient Descent
**Gradient descent** is an optimization algorithm used to find the optimal values for \( w \) and \( b \) by minimizing the loss function (usually **Mean Squared Error**, MSE).

### How Gradient Descent Works:
1. **Initialize parameters**: Start with random values for \( w \) and \( b \).
2. **Compute the loss**: Calculate the loss (e.g., MSE) based on the current values of \( w \) and \( b \).
3. **Calculate gradients**: Compute the gradients of the loss with respect to \( w \) and \( b \).
4. **Update parameters**: Adjust \( w \) and \( b \) to reduce the loss.
5. **Repeat**: Continue iterating until convergence.

### Gradient Equation for \( w \):
The gradient of the loss function with respect to the weight \( w \) is given by:

$$
\frac{\partial L}{\partial w} = \frac{2}{n} \sum_{i=1}^{n} (f(x_i) - y_i) \cdot x_i
$$

Where:
- \( L \) is the **loss function** (Mean Squared Error),
- \( n \) is the number of data points,
- \( f(x_i) \) is the predicted value for the \( i \)-th data point,
- \( y_i \) is the actual value for the \( i \)-th data point,
- \( x_i \) is the input value for the \( i \)-th data point.

## Learning Rate
The **learning rate** \( \alpha \) controls how much we adjust the parameters \( w \) and \( b \) during each update. It determines the **step size** in gradient descent.

- A **high learning rate** might cause the algorithm to overshoot the optimal values.
- A **low learning rate** makes the updates slow and may lead to prolonged convergence or getting stuck in a local minimum.

## Convergence
**Convergence** refers to the point at which the algorithm has found the optimal values for \( w \) and \( b \), or when further updates result in negligible changes to the loss function.

- Convergence is achieved when the gradient approaches zero and the model’s predictions become stable.

## Breaking Down the Gradient Equation Term by Term
Let’s break down the gradient equation for \( w \) into simpler terms:

### Gradient Equation:
$$
\frac{\partial L}{\partial w} = \frac{2}{n} \sum_{i=1}^{n} (f(x_i) - y_i) \cdot x_i
$$

1. **\( \frac{2}{n} \)**:
   - The **2** comes from the derivative of the squared error term (squared differences between predicted and actual values).
   - **\( n \)** is the number of training examples, and dividing by \( n \) **averages** the gradient over all data points.

2. **\( \sum_{i=1}^{n} \)**:
   - This is the summation symbol, meaning we add up the values for each data point from \( i = 1 \) to \( i = n \) (all training examples).

3. **\( (f(x_i) - y_i) \)**:
   - This is the **error** for the \( i \)-th data point: the difference between the predicted value \( f(x_i) \) and the actual target value \( y_i \).

4. **\( \cdot x_i \)**:
   - This is the input value \( x_i \) for the \( i \)-th data point, which is multiplied by the error. This step ensures that the error is weighted by how much the input \( x_i \) influences the prediction.

By summing the contributions from all data points and averaging them, the gradient gives us the direction in which to adjust \( w \) to minimize the loss function.

## Conclusion
- **Linear Regression** tries to fit a line to the data by adjusting the weights and bias.
- **Gradient Descent** helps minimize the loss function by iteratively adjusting the weights and bias.
- **Learning Rate** controls the size of the steps taken in each iteration.
- **Convergence** occurs when the algorithm finds the optimal values for \( w \) and \( b \), or when the loss stops decreasing.

