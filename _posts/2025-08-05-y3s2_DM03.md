---
layout: single
title:  "Data Similarity and Distance"
category: [coursework, data-mining]
tag: COMP3009
author_profile: false
sidebar:
    nav: "counts"
---

Similarity and distance measures are central to clustering, classification, and retrieval tasks. The choice of measure strongly affects algorithm performance and depends on data type and application.

- **Similarity** ‚Üí higher = more alike.  
- **Distance** ‚Üí lower = more alike.  

## Multidimensional Data

### Quantitative Data
**Lp Norms:**
- **L1 (Manhattan):** `Œ£ |xi ‚Äì yi|`
- **L2 (Euclidean):** `sqrt(Œ£ (xi ‚Äì yi)¬≤)`
- **L‚àû (Chebyshev):** `max |xi ‚Äì yi|`

Other considerations:
- **Domain relevance:** weight features differently (application-specific).
- **High-dimensionality:**  
  - Distance concentration ‚Üí all distances ‚âà same.  
  - Curse of dimensionality ‚Üí feature selection / dimensionality reduction.  
- **Locally irrelevant features:** reduce similarity quality ‚Üí use local weighting.  
- **Match-based similarity:** cosine similarity (direction not magnitude).  
- **Data distribution impact:** skewed/clustered data ‚Üí distances misleading.  
- **Nonlinear structures:** ISOMAP handles manifolds.  
- **Local distribution:** kernel density or adaptive distances.  
- **Computational issues:** need indexing, approximation for large datasets.  

### 2.2 Categorical Data
- **Simple Matching Coefficient (SMC):**  
  `(# matches) / (total attributes)`  
- Weighted versions if attributes differ in importance.  

### 2.3 Mixed Data
- Combine quantitative + categorical (e.g., **Gower‚Äôs similarity**).  

---

## 3. Text Similarity
**Representations:**
- Binary (word present/absent).
- Set-based (unique words).
- Frequency vectors (**TF**, **TF-IDF**).

**Measures:**
- **Cosine similarity:** `(X¬∑Y) / (||X||¬∑||Y||)`
- **Jaccard coefficient:** `|X‚à©Y| / |X‚à™Y|`
- **Hamming distance** for binary strings.

‚ö†Ô∏è Important: normalization (stopwords, stemming, weighting).  

---

## 4. Temporal Similarity

### 4.1 Time-Series
Challenges: scaling, translation, noise.  

**Measures:**
- Euclidean distance: direct but sensitive.
- Lp-norms: generalization.
- **Dynamic Time Warping (DTW):** elastic alignment of sequences.
- Window-based: sliding subsequence comparison.

Preprocessing: z-normalization, detrending.  

### 4.2 Discrete Sequences (Strings)
- **Edit distance (Levenshtein):** min # edits (insert, delete, substitute).  
- **Longest Common Subsequence (LCS):** length of max subsequence shared.  

---

## 5. Graph Similarity

### 5.1 Node-to-Node in One Graph
- Structural distance: shortest path lengths.  
- Random walk similarity: probability of reaching one node from another.  

### 5.2 Between Two Graphs
- **Graph edit distance:** # edits to transform one graph ‚Üí another.  
- **Spectral measures:** compare eigenvalues/eigenvectors of adjacency matrices.  

---

## 6. Supervised Similarity
- Learn task-specific similarity using labeled data.  
- **Metric learning:** adapt distances (e.g., Mahalanobis metric learning).  
- Improves classification & clustering.  

---

## 7. üöÄ Key Takeaways
- No universal best measure ‚Üí depends on **data type & task**.  
- **Quantitative data:** Lp norms, cosine, distribution-aware.  
- **Categorical/mixed:** matching coefficient, Gower‚Äôs measure.  
- **Text:** cosine, Jaccard, TF-IDF weighting.  
- **Temporal:** DTW, edit distance, subsequence matching.  
- **Graph:** structural paths, random walks, edit distance.  
- **Supervised metric learning** = powerful for specialized tasks.  
- ‚ö†Ô∏è High-dimensions & irrelevant features are dangerous ‚Üí reduce/weight features!  
