---
layout: single
title: "Linear Classfication"
category: [coursework, machine-learning]
tag: [COMP3010, ML]
author_profile: false
use_math: true
sidebar:
    nav: "counts"
---

Notes from the summary of COMP3010 Lecture 3, labs, and online resources.

## Introduction to Classification

Classification is a fundamental concept in machine learning where the goal is to assign input data to predefined **categories or classes**. Unlike regression, which predicts continuous values, **classification outputs discrete labels**.

Some common classification examples:
- Email filtering (spam vs. non-spam)
- Disease diagnosis (positive vs. negative)
- Image recognition (dog vs. cat)

This lecture transitions from regression (continuous output) to **classification** (discrete output). The key difference lies in the **nature of the target variable**.

---

## Linear Regression vs Logistic Regression

| Feature             | Linear Regression               | Logistic Regression (Classification) |
|---------------------|--------------------------------|--------------------------------------|
| **Data (Output)**   | Continuous Values             | Discrete Categories (Probabilities) |
| **Loss Function**   | Mean Squared Error (MSE)      | Cross-Entropy Loss (Binary)         |
| **Optimization**    | Gradient Descent              | Gradient Descent                    |
| **Classification**  | Not Applicable               | Probability-based (Threshold 0.5)   |
| **Primary Use**     | Predicting Continuous Values   | Classification (Binary or Multi-class) |

---

## Why is Classification Called 'Regression'?

Although **Logistic Regression** contains the word *regression*, it is used for **classification**. The term comes from its use of a **linear combination of features**, like linear regression, but with a **non-linear transformation** (sigmoid) to model probabilities.

- Outputs a **probability between 0 and 1**.
- Uses a **threshold** (usually 0.5) to assign class labels:
  - $\hat{y} \ge 0.5$ → class **1**
  - $\hat{y} < 0.5$ → class **0**

---

## Logistic Regression: The Basics

### Why Not Use Linear Regression for Classification?

Linear regression outputs unbounded values, which is unsuitable for classification. To address this, we apply the **sigmoid function** to map the output into the range **[0, 1]**.

\[
y = \frac{1}{1 + e^{-(wx + b)}}
\]

- The sigmoid maps any real value into a probability.
- The model still uses a **linear combination** of input features.
- The probability is interpreted as the likelihood of belonging to the positive class.

---

## Training Logistic Regression

### Loss Function

Using **Mean Squared Error (MSE)** with logistic regression results in a **non-convex** cost function, making optimization inefficient.

Instead, we use the **Binary Cross-Entropy Loss** (also called logistic loss):

\[
L = - \left[ y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right]
\]

- It is **convex**, ensuring better convergence.
- Emphasizes the penalty for incorrect predictions with high confidence.

### Optimization

Logistic regression is trained using **Gradient Descent**, which:
- Computes the gradient of the loss with respect to model parameters.
- Updates weights $w$ and bias $b$ iteratively.

#### Learning Rate
The **learning rate** determines the step size for each parameter update:
- Too **high** → overshooting, oscillation, or divergence.
- Too **low** → slow convergence or getting stuck in suboptimal points.

Selecting a proper learning rate is crucial and often done by **experimentation** or using **adaptive methods** like:
- **Adam**
- **RMSProp**
- **Learning rate scheduling**

---

## Understanding the Decision Boundary

A **decision boundary** is the line (or hyperplane in higher dimensions) that separates data points of different classes. In logistic regression:
- The boundary is **linear**, defined by:  
  \[
  wx + b = 0
  \]
- A point's classification depends on which side of the boundary it falls on.

---

## Support Vector Machines (SVM)

**SVM** is a powerful linear classifier that differs from logistic regression by focusing on the **margin**:

### Key Concepts:
- **Support vectors**: Data points closest to the decision boundary.
- **Margin**: Distance between the decision boundary and the support vectors.
- **Hard Margin SVM**: Enforces strict separation with no classification error.
- **Decision boundary**: Found using dot products and optimization techniques.

SVM aims to **maximize the margin**, improving generalization. It also introduces a **trigger variable $Y_i$** for representing class membership (+1 or -1).

---

## Recap of Key Concepts

| Concept               | Definition |
|------------------------|------------|
| **Classification**      | Predicting a discrete label or category. |
| **Logistic Regression** | A linear model using the sigmoid function to produce probabilities for binary classification. |
| **Sigmoid Function**    | Maps real numbers to values in the [0, 1] interval, interpreted as probabilities. |
| **Binary Cross-Entropy**| A convex loss function suitable for classification tasks. |
| **Gradient Descent**    | Optimization algorithm for updating weights to minimize the loss. |
| **Learning Rate**       | Controls the size of updates during training. |
| **Decision Boundary**   | The dividing line between classes based on model predictions. |
| **Support Vectors**     | Data points that define the margin in SVM. |
| **Margin**              | Distance between support vectors and the decision boundary. |
| **Hyperparameters**     | Parameters like learning rate, set before training begins. |

---

## Additional Notes

- In practice, classification models are extended to handle **multi-class problems** using techniques like:
  - **One-vs-All (OvA)**
  - **Softmax Regression** (Multinomial Logistic Regression)

- SVMs can also be extended to non-linear boundaries using **kernel tricks**, which transform data into higher dimensions.

---
